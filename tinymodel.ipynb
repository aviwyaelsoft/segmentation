{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47757581420788003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL.ImageOps import equalize as eq\n",
    "import random\n",
    "random.uniform(0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL.ImageOps import equalize as eq\n",
    "\n",
    "def white (img):\n",
    "    img=image.array_to_img (img)\n",
    "    img= eq(img)\n",
    "    return image.img_to_array (img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lap (img):\n",
    "    img=image.array_to_img (img).convert('RGB') \n",
    " \n",
    "    open_cv_image = np.array(img) \n",
    "# Convert RGB to BGR \n",
    "    open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
    "    laplacian = cv2.Laplacian(open_cv_image,cv2.CV_64F)\n",
    "    \n",
    "    return laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aug (img):\n",
    "    x=lap (img)\n",
    "    \n",
    "    return white(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation ,Conv2D ,BatchNormalization , Dense ,Dropout ,Flatten ,MaxPool2D ,GlobalAveragePooling2D\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aviw/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(50,100,1)),\n",
    "            Conv2D(32,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(32,(3,3), activation='relu'),\n",
    "            MaxPool2D((2,2)),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(64,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(64,(3,3), activation='relu'),\n",
    "            MaxPool2D((2,2)),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(128,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(128,(3,3), activation='relu'),\n",
    "           \n",
    "        BatchNormalization(axis=1),\n",
    "        \n",
    "        Conv2D(4 ,(3,3), border_mode='same'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 50, 100, 1)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 98, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 98, 32)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 46, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 23, 48, 32)        92        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 21, 46, 64)        84        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 19, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 22, 64)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 9, 22, 64)         36        \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 20, 128)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 20, 128)        28        \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 5, 18, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 5, 18, 128)        20        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 5, 18, 4)          4612      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 291,696\n",
      "Trainable params: 291,370\n",
      "Non-trainable params: 326\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "gen=image.ImageDataGenerator(rotation_range=30,shear_range=0.1,channel_shift_range=0.2,\n",
    "     width_shift_range=0.4 , height_shift_range=0.25)\n",
    "gen1=image.ImageDataGenerator()\n",
    "gen2=image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5820 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size=15\n",
    "\n",
    "train= gen.flow_from_directory('train', target_size=(50,100),\n",
    "                class_mode='categorical', shuffle=True, batch_size=batch_size ,color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1098 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "val= gen1.flow_from_directory('val', target_size=(50,100),\n",
    "                class_mode='categorical', shuffle=False, batch_size=batch_size , color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('tiny3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "388/388 [==============================] - 120s - loss: 0.9965 - acc: 0.5833 - val_loss: 1.5567 - val_acc: 0.2832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c12f10e48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=15\n",
    "model.fit_generator(train, steps_per_epoch=5820 /batch_size, epochs=1, \n",
    "                            validation_data=val, validation_steps=1098/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "359/358 [==============================] - 97s - loss: 1.1967 - acc: 0.4728 - val_loss: 1.5430 - val_acc: 0.2245\n",
      "Epoch 2/10\n",
      "359/358 [==============================] - 93s - loss: 1.1754 - acc: 0.4871 - val_loss: 1.1411 - val_acc: 0.5133\n",
      "Epoch 3/10\n",
      "359/358 [==============================] - 93s - loss: 1.1749 - acc: 0.4799 - val_loss: 1.4769 - val_acc: 0.2908\n",
      "Epoch 4/10\n",
      "359/358 [==============================] - 92s - loss: 1.1629 - acc: 0.4914 - val_loss: 1.2446 - val_acc: 0.4317\n",
      "Epoch 5/10\n",
      "359/358 [==============================] - 88s - loss: 1.1652 - acc: 0.4867 - val_loss: 1.2758 - val_acc: 0.4167\n",
      "Epoch 6/10\n",
      "359/358 [==============================] - 87s - loss: 1.1423 - acc: 0.5060 - val_loss: 1.3504 - val_acc: 0.3503\n",
      "Epoch 7/10\n",
      "359/358 [==============================] - 95s - loss: 1.1377 - acc: 0.5025 - val_loss: 1.3182 - val_acc: 0.4050\n",
      "Epoch 8/10\n",
      "359/358 [==============================] - 92s - loss: 1.1229 - acc: 0.5064 - val_loss: 1.6131 - val_acc: 0.1803\n",
      "Epoch 9/10\n",
      "359/358 [==============================] - 92s - loss: 1.1234 - acc: 0.5129 - val_loss: 1.1851 - val_acc: 0.5300\n",
      "Epoch 10/10\n",
      "359/358 [==============================] - 90s - loss: 1.1185 - acc: 0.5177 - val_loss: 1.7237 - val_acc: 0.1939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00155966a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr=0.1\n",
    "model.fit_generator(train, steps_per_epoch=5381/batch_size, epochs=10, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('tiny2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "359/358 [==============================] - 98s - loss: 1.1011 - acc: 0.5164 - val_loss: 1.2119 - val_acc: 0.4483\n",
      "Epoch 2/10\n",
      "359/358 [==============================] - 94s - loss: 1.0892 - acc: 0.5300 - val_loss: 1.6518 - val_acc: 0.1684\n",
      "Epoch 3/10\n",
      "359/358 [==============================] - 96s - loss: 1.0830 - acc: 0.5346 - val_loss: 1.1762 - val_acc: 0.5050\n",
      "Epoch 4/10\n",
      "359/358 [==============================] - 97s - loss: 1.0907 - acc: 0.5281 - val_loss: 1.6336 - val_acc: 0.2432\n",
      "Epoch 5/10\n",
      "359/358 [==============================] - 100s - loss: 1.0810 - acc: 0.5335 - val_loss: 1.3194 - val_acc: 0.4100\n",
      "Epoch 6/10\n",
      "359/358 [==============================] - 98s - loss: 1.0741 - acc: 0.5289 - val_loss: 1.5540 - val_acc: 0.2874\n",
      "Epoch 7/10\n",
      "359/358 [==============================] - 93s - loss: 1.0544 - acc: 0.5460 - val_loss: 1.4320 - val_acc: 0.3267\n",
      "Epoch 8/10\n",
      "359/358 [==============================] - 90s - loss: 1.0684 - acc: 0.5415 - val_loss: 1.3306 - val_acc: 0.4354\n",
      "Epoch 9/10\n",
      "359/358 [==============================] - 87s - loss: 1.0569 - acc: 0.5499 - val_loss: 1.6036 - val_acc: 0.1973\n",
      "Epoch 10/10\n",
      "359/358 [==============================] - 89s - loss: 1.0420 - acc: 0.5577 - val_loss: 1.1911 - val_acc: 0.4950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd74905898>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr=0.1\n",
    "model.fit_generator(train, steps_per_epoch=5381/batch_size, epochs=10, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "359/358 [==============================] - 102s - loss: 0.9936 - acc: 0.5783 - val_loss: 1.5053 - val_acc: 0.3300\n",
      "Epoch 2/10\n",
      "359/358 [==============================] - 100s - loss: 0.9855 - acc: 0.5790 - val_loss: 1.6863 - val_acc: 0.2092\n",
      "Epoch 3/10\n",
      "359/358 [==============================] - 101s - loss: 0.9899 - acc: 0.5811 - val_loss: 1.3213 - val_acc: 0.4000\n",
      "Epoch 4/10\n",
      "359/358 [==============================] - 100s - loss: 0.9763 - acc: 0.5948 - val_loss: 1.7733 - val_acc: 0.2024\n",
      "Epoch 5/10\n",
      "359/358 [==============================] - 103s - loss: 0.9727 - acc: 0.5955 - val_loss: 1.4593 - val_acc: 0.2967\n",
      "Epoch 6/10\n",
      "359/358 [==============================] - 100s - loss: 0.9674 - acc: 0.5974 - val_loss: 1.7492 - val_acc: 0.2262\n",
      "Epoch 7/10\n",
      "359/358 [==============================] - 102s - loss: 0.9749 - acc: 0.5948 - val_loss: 1.5642 - val_acc: 0.3050\n",
      "Epoch 8/10\n",
      "359/358 [==============================] - 98s - loss: 0.9727 - acc: 0.5980 - val_loss: 1.6837 - val_acc: 0.2228\n",
      "Epoch 9/10\n",
      "359/358 [==============================] - 97s - loss: 0.9566 - acc: 0.5976 - val_loss: 1.5718 - val_acc: 0.2517\n",
      "Epoch 10/10\n",
      "359/358 [==============================] - 100s - loss: 0.9697 - acc: 0.5922 - val_loss: 1.6090 - val_acc: 0.3083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7bfab28dd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr=0.01\n",
    "model.fit_generator(train, steps_per_epoch=5381/batch_size, epochs=10, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "160/159 [==============================] - 52s - loss: 0.9672 - acc: 0.5992 - val_loss: 1.5877 - val_acc: 0.2967\n",
      "Epoch 2/5\n",
      "160/159 [==============================] - 48s - loss: 0.9304 - acc: 0.6142 - val_loss: 1.7191 - val_acc: 0.2721\n",
      "Epoch 3/5\n",
      "160/159 [==============================] - 48s - loss: 0.9472 - acc: 0.6100 - val_loss: 1.5384 - val_acc: 0.3283\n",
      "Epoch 4/5\n",
      "160/159 [==============================] - 49s - loss: 0.9375 - acc: 0.6075 - val_loss: 1.5091 - val_acc: 0.3350\n",
      "Epoch 5/5\n",
      "160/159 [==============================] - 49s - loss: 0.9627 - acc: 0.5925 - val_loss: 1.7841 - val_acc: 0.2262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7bf9de8198>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr=0.01\n",
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001\n",
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "model.optimizer.lr=0.1\n",
    "for i in range (4,9):\n",
    "    model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)\n",
    "    model.save_weights ('new{x}.h5'.format(x=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001\n",
    "for i in range (9,15):\n",
    "    model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)\n",
    "    model.save_weights ('new{x}.h5'.format(x=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights ('newone5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = gen1.flow_from_directory('newtest', target_size=(150,300),\n",
    "                class_mode='binary', shuffle=False, batch_size=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre=model.predict_generator(test,115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = test.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mistake=0\n",
    "doubt=0\n",
    "file=[]\n",
    "file1=[]\n",
    "correct=0\n",
    "num=0\n",
    "for x in pre :\n",
    "    \n",
    "    if x[0]<0.8 and x[0]>0.2: \n",
    "        doubt+=1\n",
    "        print (x , \"doubt\")\n",
    "        file1.append (files[num])\n",
    "    else:\n",
    "        if num<0:\n",
    "            if x[0]>0.8: \n",
    "                mistake+=1\n",
    "                print (x,num)\n",
    "                file.append (files[num])\n",
    "            else : correct+=1\n",
    "        else:\n",
    "            if x[0]<0.8: \n",
    "                mistake+=1\n",
    "                print (x,num) \n",
    "                file.append (files[num])\n",
    "            else : correct+=1\n",
    "    num+=1\n",
    "    \n",
    "all=correct+mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct/all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doubt/(all+doubt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image.load_img ('test/'+files[160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = gen1.flow_from_directory('newtest', target_size=(150,300),\n",
    "                class_mode=None, shuffle=False, batch_size=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test1=np.concatenate([test.next() for i in range(test.samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=test1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=image.array_to_img (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eq(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL.ImageOps import equalize as eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv2.imread (l)\n",
    "laplacian = cv2.Laplacian(open_cv_image,cv2.CV_64F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "laplacian = cv2.Laplacian(open_cv_image,cv2.CV_64F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_cv_image =open_cv_image[:, :, ::-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_cv_image = np.array(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l.convert('RGB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=l.convert('RGB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
