{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09880644221238266"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL.ImageOps import equalize as eq\n",
    "import random\n",
    "random.uniform(0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL.ImageOps import equalize as eq\n",
    "\n",
    "def white (img):\n",
    "    img=image.array_to_img (img)\n",
    "    img= eq(img)\n",
    "    return image.img_to_array (img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lap (img):\n",
    "    img=image.array_to_img (img).convert('RGB') \n",
    " \n",
    "    open_cv_image = np.array(img) \n",
    "# Convert RGB to BGR \n",
    "    open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
    "    laplacian = cv2.Laplacian(open_cv_image,cv2.CV_64F)\n",
    "    \n",
    "    return laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aug (img):\n",
    "    x=lap (img)\n",
    "    \n",
    "    return white(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation ,Conv2D ,BatchNormalization , Dense ,Dropout ,Flatten ,MaxPool2D\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(150,300,1)),\n",
    "            Conv2D(32,(3,3), activation='relu'),\n",
    "            MaxPool2D((2,2)),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(32,(3,3), activation='relu'),\n",
    "            MaxPool2D((2,2)),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(64,(3,3), activation='relu'),\n",
    "            MaxPool2D((2,2)),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(64,(3,3), activation='relu'),\n",
    "            MaxPool2D((2,2)),\n",
    "            Flatten(),\n",
    "            Dense(500, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout (0.2),\n",
    "            Dense(500, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout (0.2),\n",
    "            Dense(4, activation='softmax')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 150, 300, 1)       600       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 148, 298, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 149, 32)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 74, 149, 32)       296       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 147, 32)       9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 73, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 36, 73, 32)        144       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 71, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 17, 35, 64)        68        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 16, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7168)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               3584500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 3,907,104\n",
      "Trainable params: 3,904,550\n",
      "Non-trainable params: 2,554\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "gen=image.ImageDataGenerator(rotation_range=30,shear_range=0.1 ,channel_shift_range=0.2 ,width_shift_range=0.4 , height_shift_range=0.25)\n",
    "gen1=image.ImageDataGenerator()\n",
    "gen2=image.ImageDataGenerator( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5820 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size=30\n",
    "\n",
    "train= gen.flow_from_directory('train', target_size=(150,300),\n",
    "                class_mode='categorical', shuffle=True, batch_size=batch_size, color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1098 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "val= gen1.flow_from_directory('val', target_size=(150,300),\n",
    "                class_mode='categorical', shuffle=False, batch_size=batch_size, color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights ('newone19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "194/194 [==============================] - 168s - loss: 1.7437 - acc: 0.3249 - val_loss: 1.5964 - val_acc: 0.2168\n",
      "Epoch 2/3\n",
      "194/194 [==============================] - 186s - loss: 1.5533 - acc: 0.4005 - val_loss: 1.5830 - val_acc: 0.2577\n",
      "Epoch 3/3\n",
      "194/194 [==============================] - 208s - loss: 1.4859 - acc: 0.4141 - val_loss: 1.7553 - val_acc: 0.2842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4acc2c7e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=30\n",
    "model.fit_generator(train, steps_per_epoch=5820/batch_size, epochs=3, \n",
    "                            validation_data=val, validation_steps=1098/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights ('copy2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "194/194 [==============================] - 244s - loss: 1.5771 - acc: 0.3460 - val_loss: 1.6861 - val_acc: 0.2778\n",
      "Epoch 2/10\n",
      "194/194 [==============================] - 268s - loss: 1.5784 - acc: 0.3481 - val_loss: 1.7408 - val_acc: 0.2413\n",
      "Epoch 3/10\n",
      "194/194 [==============================] - 251s - loss: 1.5582 - acc: 0.3672 - val_loss: 1.6625 - val_acc: 0.2823\n",
      "Epoch 4/10\n",
      "194/194 [==============================] - 236s - loss: 1.5307 - acc: 0.3704 - val_loss: 1.6791 - val_acc: 0.2778\n",
      "Epoch 5/10\n",
      "194/194 [==============================] - 202s - loss: 1.5236 - acc: 0.3668 - val_loss: 1.6132 - val_acc: 0.3188\n",
      "Epoch 6/10\n",
      "194/194 [==============================] - 208s - loss: 1.5037 - acc: 0.3799 - val_loss: 1.6380 - val_acc: 0.2978\n",
      "Epoch 7/10\n",
      "194/194 [==============================] - 207s - loss: 1.4902 - acc: 0.3825 - val_loss: 1.5364 - val_acc: 0.3297\n",
      "Epoch 8/10\n",
      "194/194 [==============================] - 215s - loss: 1.4850 - acc: 0.3749 - val_loss: 1.5441 - val_acc: 0.2769\n",
      "Epoch 9/10\n",
      "194/194 [==============================] - 207s - loss: 1.4598 - acc: 0.3863 - val_loss: 1.5741 - val_acc: 0.2723\n",
      "Epoch 10/10\n",
      "194/194 [==============================] - 209s - loss: 1.4521 - acc: 0.3885 - val_loss: 1.5105 - val_acc: 0.3124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4abcf03278>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr=0.1\n",
    "model.fit_generator(train, steps_per_epoch=5820/batch_size, epochs=10, \n",
    "                            validation_data=val, validation_steps=1098/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights ('small1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "194/194 [==============================] - 165s - loss: 1.4173 - acc: 0.4009 - val_loss: 1.5365 - val_acc: 0.2851\n",
      "Epoch 2/10\n",
      "194/194 [==============================] - 146s - loss: 1.4403 - acc: 0.4041 - val_loss: 1.5135 - val_acc: 0.2978\n",
      "Epoch 3/10\n",
      "194/194 [==============================] - 131s - loss: 1.4075 - acc: 0.4036 - val_loss: 1.4670 - val_acc: 0.3370\n",
      "Epoch 4/10\n",
      "194/194 [==============================] - 122s - loss: 1.4041 - acc: 0.4077 - val_loss: 1.4617 - val_acc: 0.3115\n",
      "Epoch 5/10\n",
      "194/194 [==============================] - 207s - loss: 1.3974 - acc: 0.4158 - val_loss: 1.4565 - val_acc: 0.3379\n",
      "Epoch 6/10\n",
      "194/194 [==============================] - 215s - loss: 1.3754 - acc: 0.4259 - val_loss: 1.5265 - val_acc: 0.2905\n",
      "Epoch 7/10\n",
      "194/194 [==============================] - 173s - loss: 1.3712 - acc: 0.4222 - val_loss: 1.5480 - val_acc: 0.2869\n",
      "Epoch 8/10\n",
      "131/194 [===================>..........] - ETA: 59s - loss: 1.3683 - acc: 0.4201"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr=0.1\n",
    "model.fit_generator(train, steps_per_epoch=5820/batch_size, epochs=10, \n",
    "                            validation_data=val, validation_steps=1098/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001\n",
    "for i in range (10,20):\n",
    "    model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)\n",
    "    model.save_weights ('newone{x}.h5'.format(x=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01\n",
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001\n",
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "model.optimizer.lr=0.1\n",
    "for i in range (4,9):\n",
    "    model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)\n",
    "    model.save_weights ('new{x}.h5'.format(x=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001\n",
    "for i in range (9,15):\n",
    "    model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)\n",
    "    model.save_weights ('new{x}.h5'.format(x=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights ('newone5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gen1.flow_from_directory('test', target_size=(150,300),\n",
    "                class_mode='binary', shuffle=False, batch_size=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre=model.predict_generator(test,238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = test.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake=0\n",
    "doubt=0\n",
    "file=[]\n",
    "file1=[]\n",
    "correct=0\n",
    "num=0\n",
    "for x in pre :\n",
    "    \n",
    "    if x[0]<0.8 and x[0]>0.2: \n",
    "        doubt+=1\n",
    "        print (x , \"doubt\")\n",
    "        file1.append (files[num])\n",
    "    else:\n",
    "        if num<118:\n",
    "            if x[0]>0.8: \n",
    "                mistake+=1\n",
    "                print (x,num)\n",
    "                file.append (files[num])\n",
    "            else : correct+=1\n",
    "        else:\n",
    "            if x[0]<0.8: \n",
    "                mistake+=1\n",
    "                print (x,num) \n",
    "                file.append (files[num])\n",
    "            else : correct+=1\n",
    "    num+=1\n",
    "    \n",
    "all=correct+mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubt/(all+doubt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.load_img ('newtest/'+files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gen1.flow_from_directory('newtest', target_size=(150,300),\n",
    "                class_mode=None, shuffle=False, batch_size=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test1=np.concatenate([test.next() for i in range(test.samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=test1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=image.array_to_img (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL.ImageOps import equalize as eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL.ImageOps import equalize as eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv2.imread (l)\n",
    "laplacian = cv2.Laplacian(open_cv_image,cv2.CV_64F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "laplacian = cv2.Laplacian(open_cv_image,cv2.CV_64F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_cv_image =open_cv_image[:, :, ::-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_cv_image = np.array(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.convert('RGB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=l.convert('RGB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
