{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111912758737864"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL.ImageOps import equalize as eq\n",
    "import random\n",
    "random.uniform(0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL.ImageOps import equalize as eq\n",
    "\n",
    "def white (img):\n",
    "    img=image.array_to_img (img)\n",
    "    img= eq(img)\n",
    "    return image.img_to_array (img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lap (img):\n",
    "    img=image.array_to_img (img).convert('RGB') \n",
    " \n",
    "    open_cv_image = np.array(img) \n",
    "# Convert RGB to BGR \n",
    "    open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
    "    laplacian = cv2.Laplacian(open_cv_image,cv2.CV_64F)\n",
    "    \n",
    "    return laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aug (img):\n",
    "    x=lap (img)\n",
    "    \n",
    "    return white(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation ,Conv2D ,BatchNormalization , Dense ,Dropout ,Flatten ,MaxPool2D ,GlobalAveragePooling2D\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aviw/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(150,300,3)),\n",
    "            Conv2D(32,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(32,(3,3), activation='relu'),\n",
    "            MaxPool2D((2,2)),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(64,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(64,(3,3), activation='relu'),\n",
    "            MaxPool2D((2,2)),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(128,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            Conv2D(128,(3,3), activation='relu'),\n",
    "           \n",
    "        BatchNormalization(axis=1),\n",
    "        \n",
    "        Conv2D(4 ,(3,3), border_mode='same'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 150, 300, 3)       600       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 148, 298, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 148, 298, 32)      592       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 146, 296, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 73, 148, 32)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 73, 148, 32)       292       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 71, 146, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 71, 146, 64)       284       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 69, 144, 64)       36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 34, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 34, 72, 64)        136       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 70, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 70, 128)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 68, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 68, 128)       120       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 30, 68, 4)         4612      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 293,772\n",
      "Trainable params: 292,696\n",
      "Non-trainable params: 1,076\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "gen=image.ImageDataGenerator(rotation_range=30,shear_range=0.1,channel_shift_range=0.2,\n",
    "     width_shift_range=0.4 , height_shift_range=0.25,preprocessing_function=lap)\n",
    "gen1=image.ImageDataGenerator(preprocessing_function=lap)\n",
    "gen2=image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5820 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size=30\n",
    "\n",
    "train= gen.flow_from_directory('train', target_size=(150,300),\n",
    "                class_mode='categorical', shuffle=True, batch_size=batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1098 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "val= gen1.flow_from_directory('val', target_size=(150,300),\n",
    "                class_mode='categorical', shuffle=False, batch_size=batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('tiny.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "194/194 [==============================] - 406s - loss: 1.3229 - acc: 0.3780 - val_loss: 1.3647 - val_acc: 0.3397\n",
      "Epoch 2/3\n",
      "194/194 [==============================] - 266s - loss: 1.2908 - acc: 0.4179 - val_loss: 1.3334 - val_acc: 0.3607\n",
      "Epoch 3/3\n",
      "194/194 [==============================] - 289s - loss: 1.2716 - acc: 0.4284 - val_loss: 1.3415 - val_acc: 0.3424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc9b5e6ba8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=30\n",
    "model.fit_generator(train, steps_per_epoch=5820 /batch_size, epochs=3, \n",
    "                            validation_data=val, validation_steps=1098/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "180/179 [==============================] - 219s - loss: 1.2541 - acc: 0.4374 - val_loss: 1.4280 - val_acc: 0.2704\n",
      "Epoch 2/10\n",
      "180/179 [==============================] - 258s - loss: 1.2403 - acc: 0.4500 - val_loss: 1.4660 - val_acc: 0.2160\n",
      "Epoch 3/10\n",
      "180/179 [==============================] - 184s - loss: 1.2315 - acc: 0.4570 - val_loss: 1.3272 - val_acc: 0.3817\n",
      "Epoch 4/10\n",
      "155/179 [========================>.....] - ETA: 22s - loss: 1.2132 - acc: 0.4692"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr=0.1\n",
    "model.fit_generator(train, steps_per_epoch=5381/batch_size, epochs=10, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('tiny2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "359/358 [==============================] - 98s - loss: 1.1011 - acc: 0.5164 - val_loss: 1.2119 - val_acc: 0.4483\n",
      "Epoch 2/10\n",
      "359/358 [==============================] - 94s - loss: 1.0892 - acc: 0.5300 - val_loss: 1.6518 - val_acc: 0.1684\n",
      "Epoch 3/10\n",
      "359/358 [==============================] - 96s - loss: 1.0830 - acc: 0.5346 - val_loss: 1.1762 - val_acc: 0.5050\n",
      "Epoch 4/10\n",
      "359/358 [==============================] - 97s - loss: 1.0907 - acc: 0.5281 - val_loss: 1.6336 - val_acc: 0.2432\n",
      "Epoch 5/10\n",
      "359/358 [==============================] - 100s - loss: 1.0810 - acc: 0.5335 - val_loss: 1.3194 - val_acc: 0.4100\n",
      "Epoch 6/10\n",
      "359/358 [==============================] - 98s - loss: 1.0741 - acc: 0.5289 - val_loss: 1.5540 - val_acc: 0.2874\n",
      "Epoch 7/10\n",
      "359/358 [==============================] - 93s - loss: 1.0544 - acc: 0.5460 - val_loss: 1.4320 - val_acc: 0.3267\n",
      "Epoch 8/10\n",
      "359/358 [==============================] - 90s - loss: 1.0684 - acc: 0.5415 - val_loss: 1.3306 - val_acc: 0.4354\n",
      "Epoch 9/10\n",
      "359/358 [==============================] - 87s - loss: 1.0569 - acc: 0.5499 - val_loss: 1.6036 - val_acc: 0.1973\n",
      "Epoch 10/10\n",
      "359/358 [==============================] - 89s - loss: 1.0420 - acc: 0.5577 - val_loss: 1.1911 - val_acc: 0.4950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd74905898>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr=0.1\n",
    "model.fit_generator(train, steps_per_epoch=5381/batch_size, epochs=10, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "359/358 [==============================] - 97s - loss: 1.0549 - acc: 0.5461 - val_loss: 1.7604 - val_acc: 0.1480\n",
      "Epoch 2/10\n",
      "359/358 [==============================] - 93s - loss: 1.0365 - acc: 0.5612 - val_loss: 1.1719 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "359/358 [==============================] - 94s - loss: 1.0281 - acc: 0.5653 - val_loss: 1.8208 - val_acc: 0.1633\n",
      "Epoch 4/10\n",
      "359/358 [==============================] - 92s - loss: 1.0274 - acc: 0.5643 - val_loss: 1.2987 - val_acc: 0.4050\n",
      "Epoch 5/10\n",
      "359/358 [==============================] - 93s - loss: 1.0185 - acc: 0.5642 - val_loss: 1.7067 - val_acc: 0.2364\n",
      "Epoch 6/10\n",
      "359/358 [==============================] - 92s - loss: 1.0212 - acc: 0.5675 - val_loss: 1.5090 - val_acc: 0.2833\n",
      "Epoch 7/10\n",
      "359/358 [==============================] - 91s - loss: 0.9991 - acc: 0.5794 - val_loss: 1.5523 - val_acc: 0.2993\n",
      "Epoch 8/10\n",
      "359/358 [==============================] - 92s - loss: 1.0215 - acc: 0.5588 - val_loss: 1.6996 - val_acc: 0.2925\n",
      "Epoch 9/10\n",
      "359/358 [==============================] - 94s - loss: 0.9866 - acc: 0.5911 - val_loss: 1.3371 - val_acc: 0.4200\n",
      "Epoch 10/10\n",
      "359/358 [==============================] - 95s - loss: 0.9923 - acc: 0.5803 - val_loss: 1.6798 - val_acc: 0.2041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd74837f98>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr=0.01\n",
    "model.fit_generator(train, steps_per_epoch=5381/batch_size, epochs=10, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01\n",
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001\n",
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "model.optimizer.lr=0.1\n",
    "for i in range (4,9):\n",
    "    model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)\n",
    "    model.save_weights ('new{x}.h5'.format(x=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001\n",
    "for i in range (9,15):\n",
    "    model.fit_generator(train, steps_per_epoch=2387/batch_size, epochs=5, \n",
    "                            validation_data=val, validation_steps=594/batch_size)\n",
    "    model.save_weights ('new{x}.h5'.format(x=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights ('newone5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = gen1.flow_from_directory('newtest', target_size=(150,300),\n",
    "                class_mode='binary', shuffle=False, batch_size=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre=model.predict_generator(test,115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = test.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mistake=0\n",
    "doubt=0\n",
    "file=[]\n",
    "file1=[]\n",
    "correct=0\n",
    "num=0\n",
    "for x in pre :\n",
    "    \n",
    "    if x[0]<0.8 and x[0]>0.2: \n",
    "        doubt+=1\n",
    "        print (x , \"doubt\")\n",
    "        file1.append (files[num])\n",
    "    else:\n",
    "        if num<0:\n",
    "            if x[0]>0.8: \n",
    "                mistake+=1\n",
    "                print (x,num)\n",
    "                file.append (files[num])\n",
    "            else : correct+=1\n",
    "        else:\n",
    "            if x[0]<0.8: \n",
    "                mistake+=1\n",
    "                print (x,num) \n",
    "                file.append (files[num])\n",
    "            else : correct+=1\n",
    "    num+=1\n",
    "    \n",
    "all=correct+mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct/all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doubt/(all+doubt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image.load_img ('test/'+files[160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = gen1.flow_from_directory('newtest', target_size=(150,300),\n",
    "                class_mode=None, shuffle=False, batch_size=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test1=np.concatenate([test.next() for i in range(test.samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=test1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=image.array_to_img (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eq(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL.ImageOps import equalize as eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv2.imread (l)\n",
    "laplacian = cv2.Laplacian(open_cv_image,cv2.CV_64F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "laplacian = cv2.Laplacian(open_cv_image,cv2.CV_64F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_cv_image =open_cv_image[:, :, ::-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_cv_image = np.array(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l.convert('RGB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=l.convert('RGB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
